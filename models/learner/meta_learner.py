import pandas as pd
import numpy as np

from models.learner import Learner
from models.parameters.learner import Classifier
from models.learner.weak_learner import WeakLearner

# TODO: Add better support for intelligent data catching / logs in training process to create the graphics
class MetaLearner(Learner):
  epochs = 20
  batch_size = 128

  def __init__(self, classifier_params: Classifier):
    super().__init__(classifier_params)

    self.weak_learners: list[WeakLearner] = []
    self.embeddings_trained = np.array([])
    self.predictions_per_batch = np.array([])

  def add_weak_learner(self, weak_learner: WeakLearner):
    self.weak_learners.append(weak_learner)

  def generate_embeddings(self, data: list | None = None):
    return np.array([weak_learner.predict(data) for weak_learner in self.weak_learners])
  
  def _concatenate_embeddings(self):
    """ Concatenate the embeddings of each one of the weak learners associated to the meta learner

    What it does, is that for each one of the N embeddings generated by the weak learners, 
    it concatenates them in a list of N elements, where each element is also a list of the
    concatenated embeddings of each weak learner

    For example, given that we obtain the embeddings of size 3 of N weak learners, the output will be:
    .. code-block:: python

    [
     [embeddings_1_1, embeddings_2_1, embeddings_3_1],
     [embeddings_1_2, embeddings_2_2, embeddings_3_2],
     ...
     [embeddings_1_N, embeddings_2_N, embeddings_3_N]
    ]
    """
    embeddings = [weak_learner.embeddings for weak_learner in self.weak_learners]

    return [
      np.concatenate(
        [
        embeddings[j][i] for j in range(len(embeddings))
        ], 
        axis=0
      ) for i in range(len(embeddings[0]))
    ]
  
  # TODO: this whole funciton should be optimize to obtain it in a better way
  def train_model(self, dataframe: pd.DataFrame):
    epoch = 0

    classes_qty = list(range(len(dataframe["target"][0]))) # TODO: this has to be a list of number from 0 to the max number of classes (-1). OPTIMIZE

    weak_learners_predictions = self._concatenate_embeddings()
    train_samples = len(weak_learners_predictions)
    while epoch < self.epochs:
      mini_batch_index = 0
      while True:
        self.classifier.partial_fit(weak_learners_predictions, dataframe["target"].to_list(), classes=classes_qty)
        mini_batch_index += self.batch_size

        if mini_batch_index >= train_samples:
          break

      self.predictions_per_batch = np.append(self.predictions_per_batch, self.classifier.predict(weak_learners_predictions))
      epoch += 1